{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# blabla\n",
    "\n",
    "<div class=\"cite2c-biblio\"></div><div class=\"cite2c-biblio\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import csv, datetime, time, json\n",
    "from zipfile import ZipFile\n",
    "from os.path import expanduser, exists\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "from io import open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proudfsdf\n"
     ]
    }
   ],
   "source": [
    "print('proudfsdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "QUESTION_PAIRS_FILE_URL = 'http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv'\n",
    "QUESTION_PAIRS_FILE = 'data/train.csv'\n",
    "GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "GLOVE_ZIP_FILE = 'data/glove.840B.300d.zip'\n",
    "GLOVE_FILE = 'data/glove.840B.300d.txt'\n",
    "Q1_TRAINING_DATA_FILE = 'data/q1_train.npy'\n",
    "Q2_TRAINING_DATA_FILE = 'data/q2_train.npy'\n",
    "LABEL_TRAINING_DATA_FILE = 'data/label_train.npy'\n",
    "WORD_EMBEDDING_MATRIX_FILE = 'data/word_embedding_matrix.npy'\n",
    "NB_WORDS_DATA_FILE = 'data/nb_words.json'\n",
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "EMBEDDING_DIM = 300\n",
    "MODEL_WEIGHTS_FILE = 'data/question_pairs_weights.h5'\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447\n",
    "NB_EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/glove.840B.300d.txt\n",
      "Processing data/train.csv\n",
      "Question pairs: 404290\n",
      "Word embeddings: 2196016\n",
      "Null word embeddings: 29512\n",
      "Shape of question1 data tensor: (404290, 25)\n",
      "Shape of question2 data tensor: (404290, 25)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nacim/anaconda2/lib/python2.7/site-packages/keras/preprocessing/text.py:89: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "if exists(Q1_TRAINING_DATA_FILE) and exists(Q2_TRAINING_DATA_FILE) and exists(LABEL_TRAINING_DATA_FILE) and exists(NB_WORDS_DATA_FILE) and exists(WORD_EMBEDDING_MATRIX_FILE):\n",
    "    q1_data = np.load(open(Q1_TRAINING_DATA_FILE, 'rb'))\n",
    "    q2_data = np.load(open(Q2_TRAINING_DATA_FILE, 'rb'))\n",
    "    labels = np.load(open(LABEL_TRAINING_DATA_FILE, 'rb'))\n",
    "    word_embedding_matrix = np.load(open(WORD_EMBEDDING_MATRIX_FILE, 'rb'))\n",
    "    with open(NB_WORDS_DATA_FILE, 'r') as f:\n",
    "        nb_words = json.load(f)\n",
    "        print(nb_words)\n",
    "else:\n",
    "    if not exists(GLOVE_ZIP_FILE):\n",
    "        zipfile = ZipFile(get_file(GLOVE_ZIP_FILE, GLOVE_ZIP_FILE_URL))\n",
    "        zipfile.extract(GLOVE_FILE, path='data')\n",
    "\n",
    "    print(\"Processing\", GLOVE_FILE)\n",
    "    \n",
    "    \n",
    "    if not exists(QUESTION_PAIRS_FILE):\n",
    "        get_file(QUESTION_PAIRS_FILE, QUESTION_PAIRS_FILE_URL)\n",
    "\n",
    "    print(\"Processing\", QUESTION_PAIRS_FILE)\n",
    "\n",
    "    question1 = []\n",
    "    question2 = []\n",
    "    is_duplicate = []\n",
    "    \n",
    "    csvfile = pd.read_csv(QUESTION_PAIRS_FILE).fillna(' ')\n",
    "    question1 = csvfile['question1'].tolist()\n",
    "    question2 = csvfile['question2'].tolist()\n",
    "    is_duplicate = csvfile['is_duplicate'].tolist()    \n",
    "    \n",
    "    print('Question pairs: %d' % len(question1))\n",
    "    \n",
    "    questions = question1 + question2\n",
    "    tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(questions)\n",
    "    question1_word_sequences = tokenizer.texts_to_sequences(question1)\n",
    "    question2_word_sequences = tokenizer.texts_to_sequences(question2)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    \n",
    "    \n",
    "    embeddings_index = {}\n",
    "    with open(GLOVE_FILE, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "\n",
    "    print('Word embeddings: %d' % len(embeddings_index))\n",
    "\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            word_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n",
    "\n",
    "    q1_data = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    q2_data = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = np.array(is_duplicate, dtype=int)\n",
    "    print('Shape of question1 data tensor:', q1_data.shape)\n",
    "    print('Shape of question2 data tensor:', q2_data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    np.save(open(Q1_TRAINING_DATA_FILE, 'wb'), q1_data)\n",
    "    np.save(open(Q2_TRAINING_DATA_FILE, 'wb'), q2_data)\n",
    "    np.save(open(LABEL_TRAINING_DATA_FILE, 'wb'), labels)\n",
    "    np.save(open(WORD_EMBEDDING_MATRIX_FILE, 'wb'), word_embedding_matrix)\n",
    "    \n",
    "    with open(NB_WORDS_DATA_FILE,'w',encoding=\"utf-8\") as f:\n",
    "        r = {'nb_words': nb_words}\n",
    "        f.write(unicode(json.dumps(r, ensure_ascii=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nacim/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:12: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/nacim/anaconda2/lib/python2.7/site-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2017-06-02 01:52:23.368548\n",
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/25\n",
      "  4096/327474 [..............................] - ETA: 346s - loss: 0.8657 - acc: 0.5393"
     ]
    }
   ],
   "source": [
    "Q1 = Sequential()\n",
    "Q1.add(Embedding(nb_words + 1, EMBEDDING_DIM, weights=[word_embedding_matrix],\n",
    "                 input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "Q1.add(TimeDistributed(Dense(EMBEDDING_DIM, activation='relu')))\n",
    "Q1.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, )))\n",
    "Q2 = Sequential()\n",
    "Q2.add(Embedding(nb_words + 1, EMBEDDING_DIM, weights=[word_embedding_matrix],\n",
    "                 input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "Q2.add(TimeDistributed(Dense(EMBEDDING_DIM, activation='relu')))\n",
    "Q2.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, )))\n",
    "model = Sequential()\n",
    "model.add(Merge([Q1, Q2], mode='concat'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=[ \"accuracy\" ])\n",
    "\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "\n",
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "\n",
    "t0 = time.time()\n",
    "history = model.fit([Q1_train, Q2_train], \n",
    "                    y_train, \n",
    "                    batch_size=2048, shuffle=True,\n",
    "                    class_weight={0: 1.309028344, 1: 0.472001959},\n",
    "                    nb_epoch=NB_EPOCHS, \n",
    "                    validation_split=VALIDATION_SPLIT, \n",
    "                    verbose=1, \n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))\n",
    "\n",
    "model.load_weights(MODEL_WEIGHTS_FILE)\n",
    "\n",
    "loss, accuracy, precision, recall, fbeta_score = model.evaluate([Q1_test, Q2_test], y_test)\n",
    "print('')\n",
    "print('loss      = {0:.4f}'.format(loss))\n",
    "print('accuracy  = {0:.4f}'.format(accuracy))\n",
    "print('precision = {0:.4f}'.format(precision))\n",
    "print('recall    = {0:.4f}'.format(recall))\n",
    "print('F         = {0:.4f}'.format(fbeta_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
